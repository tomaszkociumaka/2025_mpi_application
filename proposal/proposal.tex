
\documentclass[a4paper,11pt]{article}
\usepackage{fancyhdr}
\usepackage[margin=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage[OT4]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{setspace}
\usepackage{fontspec}
\usepackage{comment}
\usepackage{hyperref}
\setmainfont{Times New Roman}
\doublespacing
\hypersetup{
    colorlinks = true,
    citecolor = OliveGreen
}
\hypersetup{pdftitle={Tomasz Kociumaka: Research Proposal}}

\usepackage{titlesec}

\titleformat{\section}  % which section command to format
  {\fontsize{11}{11}\bfseries} % format for whole line
  {\thesection.} % how to show number
  {.5em} % space between number and text
  {} % formatting for just the text
  [] % formatting for after the text

\newcommand{\eps}{\varepsilon}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\lhead{Tomasz Kociumaka}
\rhead{Research Proposal}

\fancypagestyle{firststyle}
{
  \renewcommand{\headrulewidth}{0pt}
  \lhead{Tomasz Kociumaka}
  \rhead{\today}
  \chead{\textbf{Research Proposal}}
}

\newcommand{\Oh}{\mathcal{O}}
\newcommand{\Ohtilde}{\tilde{\mathcal{O}}}




\begin{document}
\thispagestyle{firststyle}

Strings constitute a basic data type used to represent diverse objects such as natural language texts, biological sequences, computer files, and time series. 
The overarching goal of my research program is to design efficient algorithms and data structures for processing strings. 
This direction lies at the heart of theoretical computer science, yet it also connects to applied areas such as bioinformatics and data mining. 
The explosive growth of sequential data presents immense challenges motivating new paradigms, while persistent progress in theory yields techniques that allow tackling decades-old open questions.

My agenda focuses on fundamental theoretical problems with strong practical motivation, approached from various angles such as fine-grained complexity, dynamic algorithms, streaming algorithms, quandum computing, and computation over compressed data. 
The long-term vision is to develop a unified toolkit of techniques that can be leveraged to tackle increasingly complex problems. 
My previous work has already broken long-standing barriers, and a dedicated Max Planck Research Group will enable ambitious projects of larger scale and scope. 
The program is structured around three interconnected pillars.

\section{Approximate Pattern Matching}

Approximate pattern matching asks to identify fragments of a text similar to a given pattern, under a limit of $k$ mismatches or errors. 
Despite its classical status and wide applications, the complexity of both variants remains unresolved. 
For mismatches, major progress can only be achieved using fast matrix multiplication~\cite{GU18}; 
for errors, we can reasonably hope for an $\Ohtilde(n+\tfrac{n}{m}k^{2})$-time\footnote{Henceforth, the $\Ohtilde(\cdot)$ notation hides factors polylogarithmic in the text length $n$, where applicable, the pattern length $m$.} algorithm, but fastest known algorithm takes $\Ohtilde(n+\tfrac{n}{m}k^{3.5})$ time~\cite{CKW22}. 
It comes from my recent work that improves upon a long-standing bound of $\Oh(n+\tfrac{n}{m}k^4)$~\cite{CH98}.
I believe that we already have the necessary tools to achieve an $\Ohtilde(n+\tfrac{n}{m}k^{3})$-time solution, but the resulting procedure is very complicated, and the scale of this project highlights the need for a larger team. 
The ultimate long-term question is to determine the true dependence on $k$, between quadratic and cubic.
Preliminary evidence suggests subcubic dependence for most patterns (unless the pattern compresses to size $\Ohtilde(k)$ with methods such as Lempel--Ziv parsing).  

A complementary goal is to understand space complexity in the streaming model, where the input is revealed from left to right.
For errors, the state-of-the-art complexity of $\Ohtilde(k^2)$~\cite{BK23a} is obtained by using reduction to my $\Ohtilde(k)$-space algorithm for mismatches~\cite{CKP19}. The long-term goal is to achieve $\Ohtilde(k)$ for edits as well, and the recent advances for sketching~\cite{KS24} brought us slightly closer to this target.

The key ideas behind my results listed above arose during the study of the problem in various non-standard settings (beyond streaming, also quantum and fully compressed), and I intend to keep them as stepping stones toward the goal of settling the fine-grained complexity of approximate pattern matching.


\section{Edit Distance and Related Measures}

The edit distance, the minimum number of edits to transform one string into another, is the canonical similarity measure. 
The exact complexity in the standard setting is well understood, yet many questions remain, and the most pressing of them concern edit distance approximation:
No $3$-approximation runs in truly subquadratic time, but even a $(1+\eps)$-approximation in truly sublinear time may be possible (unless the edit distance $k$ is subpolynomial in the string length $n$).
My contributions include state-of-the-art dynamic~\cite{KMS23} and sublinear-time~\cite{BCFK24,GKKS22} approximations, both with subpolynomial ratios.
Unfortunately, the running time is sublinear only for $k \lesssim \sqrt{n}$, and it is an interesting question if fine-grained hypotheses could be used to separate the time and query complexity in the case of larger distances.


Another direction asks whether exact edit distance can be computed efficiently when the strings compress to size $z$ (e.g., by LZ77). 
The best running time is $\Ohtilde(z\sqrt{kn})$~\cite{GKLS22}, and even modest improvements could impact dynamic edit distance algorithms~\cite{GK25}.  

Beyond the classical setting, I have developed efficient algorithms for related measures, including weighted edit distance, tree edit distance, and dynamic time warping~\cite{DGHKS23,Kus19}. 
While the first two admit nearly optimal algorithms, dynamic time warping remains poorly understood and may require new conditional lower bounds.  

Finally, I plan to explore stronger hardness results. 
Current lower bounds based on Orthogonal Vectors~\cite{BI18} explain little about small-space or distributed settings; 
only quantum lower bounds~\cite{BPS21} show stronger limitations. 
Extending these techniques could uncover new sources of hardness.  

Overall, the goal is to clarify the complexity landscape for edit distance and its variants, advancing both exact and approximate computation.

\section{Small-Space Text Indexing}

Text indexes are data structures supporting fast queries on large collections. 
My work introduced \emph{synchronizing sets}~\cite{KK19}, now central components of many indexes, and enabled progress on long-standing problems such as LZ77 factorization~\cite{KK24}. 
More recently, I designed compressed indexes of essentially optimal size supporting suffix array functionality~\cite{KK23}.  

Despite their theoretical elegance, optimal-size indexes are currently impractical: they are highly complex and incur large polylogarithmic query times. 
A priority of my proposal is to optimize and simplify these constructions, bridging the gap between theory and applications.  

Another core direction is to understand optimal trade-offs between index size and query time. 
Existing lower bounds apply only to isolated cases~\cite{VY13}, leaving broad regimes unexplored. 
My ongoing work provides tight characterizations for random access when allowing slightly more space than the optimal compressed representation. 
Two long-standing open questions remain: whether random access is possible in $o(\log n)$ time within grammar-compressed space, or in polylogarithmic time and LZ77 space.  

The overarching aim is to establish a comprehensive theory of space--time trade-offs for indexing, from highly compressible to poorly compressible texts, and to simplify optimal indexes toward practical implementations.

\section*{Closing}

Together, these three pillars---approximate pattern matching, edit distance and related measures, and small-space text indexing---define a coherent program advancing the foundations of string algorithms. 
The proposed research combines deep theoretical challenges with high practical relevance, and will establish powerful techniques with broad applicability across computer science.


\singlespacing 
\let\oldthebibliography\thebibliography
\let\endoldthebibliography\endthebibliography
\renewenvironment{thebibliography}[1]{\small
  \begin{oldthebibliography}{#1}
    \setlength{\itemsep}{0em}
    \setlength{\parskip}{0em}
}
{
  \end{oldthebibliography}
}

{ 
\bibliography{proposal}
\bibliographystyle{habbrv} 
}
\end{document}
 
